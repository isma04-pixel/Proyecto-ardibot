from langchain_chroma import Chroma
from langchain_ollama import ChatOllama
from langchain_community.embeddings.fastembed import FastEmbedEmbeddings
from langchain_community.cache import InMemoryCache
from langchain.globals import set_llm_cache
from langchain.chains import create_retrieval_chain
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain_core.prompts import ChatPromptTemplate
import logging
import os
import time
import hashlib
import glob
import shutil

from .utils import load_and_split_pdf

# Configurar logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Habilitar cach√© para los LLMs
set_llm_cache(InMemoryCache())

# Cach√© de respuestas para consultas id√©nticas
RESPONSE_CACHE = {}
# Tama√±o m√°ximo de cach√© para evitar consumo excesivo de memoria
MAX_CACHE_SIZE = 100

# Directorios locales que no se trackean en git
DATA_DIR = "local_data"
PERSIST_DIR = os.path.join(DATA_DIR, "chroma_db")
EMBEDDING_CACHE_DIR = os.path.join(DATA_DIR, "embedding_cache")

# Par√°metros de b√∫squeda simplificados para garantizar compatibilidad
RETRIEVER_SEARCH_KWARGS = {
    "k": 5  # N√∫mero de documentos a recuperar
}

def ingest(pdf_paths=None):
    """
    Ingiere uno o varios documentos PDF para construir la base de conocimientos.
    
    Args:
        pdf_paths: Puede ser una ruta a un archivo PDF espec√≠fico, una lista de rutas, 
                  o None para procesar todos los PDFs en el directorio data/reglamento/
    
    Returns:
        str: Mensaje con el resultado de la ingesti√≥n
    """
    try:
        start_time = time.time()
        
        # Si no se especifican rutas, procesar todos los PDFs en el directorio reglamento
        if pdf_paths is None:
            pdf_paths = glob.glob("data/reglamento/*.pdf")
            if not pdf_paths:
                logger.error("No se encontraron archivos PDF en data/reglamento/")
                return "Error: No se encontraron archivos PDF para ingestar"
        
        # Si es una sola ruta, convertirla a lista
        if isinstance(pdf_paths, str):
            pdf_paths = [pdf_paths]
        
        # Verificar que los archivos existan
        for pdf_path in pdf_paths:
            if not os.path.exists(pdf_path):
                logger.error(f"El archivo {pdf_path} no existe")
                return f"Error: El archivo {pdf_path} no existe"
        
        # Asegurar que los directorios existan
        os.makedirs(DATA_DIR, exist_ok=True)
        os.makedirs(EMBEDDING_CACHE_DIR, exist_ok=True)
        
        # Reiniciar la base de datos vectorial para asegurar una ingesti√≥n limpia
        if os.path.exists(PERSIST_DIR):
            logger.info(f"Eliminando base de datos vectorial existente: {PERSIST_DIR}")
            try:
                shutil.rmtree(PERSIST_DIR)
                logger.info("Base de datos vectorial eliminada correctamente")
            except Exception as e:
                logger.error(f"Error al eliminar base de datos: {str(e)}")
                # Continuar con la operaci√≥n
                
        # Crear el directorio vac√≠o
        os.makedirs(PERSIST_DIR, exist_ok=True)
        
        # Procesar cada PDF y recolectar todos los chunks
        all_chunks = []
        total_chunks = 0
        
        for pdf_path in pdf_paths:
            logger.info(f"Procesando documento: {pdf_path}")
            chunks = load_and_split_pdf(pdf_path)
            all_chunks.extend(chunks)
            total_chunks += len(chunks)
            logger.info(f"Documento {pdf_path} dividido en {len(chunks)} chunks")
        
        logger.info(f"Total de documentos procesados: {len(pdf_paths)}")
        logger.info(f"Total de chunks: {total_chunks}")
        
        # Configurar embeddings
        embedding = FastEmbedEmbeddings(
            model_name="sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2",
            cache_dir=EMBEDDING_CACHE_DIR
        )
        
        # Crear la base de vectores
        Chroma.from_documents(
            documents=all_chunks,
            embedding=embedding,
            persist_directory=PERSIST_DIR,
            collection_metadata={"hnsw:space": "cosine"}
        )
        
        # Limpiar cach√© para reflejar nueva base de conocimiento
        RESPONSE_CACHE.clear()
        
        elapsed_time = time.time() - start_time
        logger.info(f"Ingesti√≥n completada en {elapsed_time:.2f} segundos")
        
        return f"Ingesti√≥n completada en {elapsed_time:.2f} segundos. Se procesaron {total_chunks} fragmentos de {len(pdf_paths)} documentos."
    except Exception as e:
        logger.error(f"Error durante la ingesti√≥n: {str(e)}", exc_info=True)
        return f"Error durante la ingesti√≥n: {str(e)}"

def get_query_hash(query):
    """Genera un hash √∫nico para la consulta para usar como clave de cach√©"""
    return hashlib.md5(query.lower().strip().encode('utf-8')).hexdigest()

def ask(query: str):
    """
    Responde preguntas usando la base de conocimiento.
    """
    try:
        logger.info(f"üîç Procesando consulta: {query}")
        
        # Verificar cach√©
        query_hash = get_query_hash(query)
        if query_hash in RESPONSE_CACHE:
            logger.info("‚úÖ Respuesta encontrada en cach√©")
            return RESPONSE_CACHE[query_hash]
        
        # Verificar que exista la base de datos
        if not os.path.exists(PERSIST_DIR):
            logger.error("‚ùå Directorio de persistencia no existe")
            return "‚ùå Base de conocimiento no encontrada. Por favor, ejecuta primero la ingesti√≥n de documentos."
        
        logger.info("üì• Configurando embeddings...")
        # Configurar embeddings y vectorstore
        embeddings = FastEmbedEmbeddings(
            model_name="sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2",
            cache_dir=EMBEDDING_CACHE_DIR
        )
        
        logger.info("üì• Cargando vectorstore...")
        vectorstore = Chroma(
            persist_directory=PERSIST_DIR,
            embedding_function=embeddings
        )
        
        # Verificar que hay documentos
        logger.info("üîç Verificando documentos en vectorstore...")
        try:
            test_docs = vectorstore.similarity_search("universidad", k=1)
            logger.info(f"üìÑ Documentos encontrados en prueba: {len(test_docs)}")
            if not test_docs:
                return "‚ùå La base de conocimiento est√° vac√≠a. Por favor, reingesta los documentos."
        except Exception as e:
            logger.error(f"‚ùå Error en similarity_search: {str(e)}")
            return f"‚ùå Error accediendo a la base de conocimiento: {str(e)}"
        
        # Configurar retriever
        logger.info("üîß Configurando retriever...")
        retriever = vectorstore.as_retriever(search_kwargs={"k": 5})
        
        # Probamos el retriever
        logger.info("üîç Probando retriever...")
        retrieved_docs = retriever.get_relevant_documents(query)
        logger.info(f"üìÑ Documentos recuperados: {len(retrieved_docs)}")
        
        if not retrieved_docs:
            return "‚ùå No se encontr√≥ informaci√≥n relevante para tu pregunta en los documentos."
        
        # Prompt
        prompt = ChatPromptTemplate.from_template("""
        Eres Ardy, un asistente de la Universidad de Ibagu√©. Responde amablemente 
        bas√°ndote SOLO en la informaci√≥n proporcionada:

        {context}

        Pregunta: {input}

        Respuesta (s√© claro y conciso):
        """)
        
        # Modelo
        logger.info("ü§ñ Configurando modelo Ollama...")
        model = ChatOllama(
            model="llama3",
            temperature=0.1,
            num_predict=512,
        )
        
        # Crear cadena
        logger.info("‚õìÔ∏è Creando cadenas...")
        document_chain = create_stuff_documents_chain(model, prompt)
        retrieval_chain = create_retrieval_chain(retriever, document_chain)
        
        # Ejecutar
        logger.info("üöÄ Invocando cadena...")
        response = retrieval_chain.invoke({"input": query})
        
        logger.info(f"üìù Respuesta recibida: {len(response)} elementos")
        logger.info(f"üìù Keys en respuesta: {list(response.keys())}")
        
        answer = response.get("answer", "No se pudo generar respuesta")
        
        # Guardar en cach√©
        RESPONSE_CACHE[query_hash] = answer
        
        logger.info("‚úÖ Consulta procesada exitosamente")
        return answer
        
    except Exception as e:
        error_msg = f"‚ùå Error procesando consulta: {str(e)}"
        logger.error(error_msg, exc_info=True)
        import traceback
        logger.error(traceback.format_exc())
        return f"Lo siento, ha ocurrido un error: {str(e)}"